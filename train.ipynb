{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from efficientdet.dataset import CocoDataset, collater, get_train_transform, get_val_transform\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, boolean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU can be used.\n",
      "Number of CUDA devices: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available. GPU can be used.\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Only CPU will be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "project_name = 'taco'  # also the folder name of the dataset that under data_path folder\n",
    "train_set = 'train'\n",
    "val_set = 'val'\n",
    "num_gpus = 1\n",
    "\n",
    "# replace with your own path\n",
    "data_path = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(f'projects/{project_name}.yml', 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "obj_list = params['obj_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p pre-trained_weights\n",
    "!wget -nc https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth -O pre-trained_weights/efficientdet-d0.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "[Warning] Ignoring Error(s) in loading state_dict for EfficientDetBackbone:\n",
      "\tsize mismatch for classifier.header.pointwise_conv.conv.weight: copying a param with shape torch.Size([810, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([540, 64, 1, 1]).\n",
      "\tsize mismatch for classifier.header.pointwise_conv.conv.bias: copying a param with shape torch.Size([810]) from checkpoint, the shape in current model is torch.Size([540]).\n",
      "[Warning] Don't panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.\n",
      "[Info] loaded weights: efficientdet-d0.pth, resuming checkpoint from step: 0\n",
      "[Info] freezed backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\train.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ret = model.load_state_dict(torch.load(weights_path), strict=False)\n",
      "\n",
      "  0%|          | 0/65 [00:00<?, ?it/s]\n",
      "  0%|          | 0/65 [01:05<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\train.py\", line 323, in <module>\n",
      "    train(opt)\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\train.py\", line 208, in train\n",
      "    for iter, data in enumerate(progress_bar):\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1344, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1370, in _process_data\n",
      "    data.reraise()\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\_utils.py\", line 706, in reraise\n",
      "    raise exception\n",
      "ValueError: Caught ValueError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 309, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\efficientdet\\dataset.py\", line 56, in __getitem__\n",
      "    transformed = self.transform(image=img, bboxes=boxes, category_ids=labels)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 345, in __call__\n",
      "    self.preprocess(data)\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 379, in preprocess\n",
      "    p.preprocess(data)\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\utils.py\", line 127, in preprocess\n",
      "    data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\utils.py\", line 141, in check_and_convert\n",
      "    return process_func(data, image_shape)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 155, in convert_to_albumentations\n",
      "    return convert_bboxes_to_albumentations(data, self.params.format, image_shape, check_validity=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\augmentations\\utils.py\", line 180, in wrapper\n",
      "    return func(array, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 294, in convert_bboxes_to_albumentations\n",
      "    check_bboxes(converted_bboxes)\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\augmentations\\utils.py\", line 180, in wrapper\n",
      "    return func(array, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\itsd\\Yet-Another-EfficientDet-Pytorch\\.venv\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 371, in check_bboxes\n",
      "    raise ValueError(\n",
      "ValueError: Expected y_max for bbox [0.321875  0.559375  0.8150469 1.2454859 0.       ] to be in the range [0.0, 1.0], got 1.2454859018325806.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train head only\n",
    "!python train.py -c 0 -p taco --head_only True --lr 1e-3 --batch_size 16 --load_weights pre-trained_weights/efficientdet-d0.pth --num_epochs 50 --save_interval 10 --debug True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train whole model\n",
    "!python train.py -c 0 -p taco --head_only False --lr 1e-4 --batch_size 8 --load_weights logs/taco/efficientdet-d0_49_3250.pth --num_epochs 100 --save_interval 50 --debug True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "weight_file = glob.glob('logs/taco/*.pth')\n",
    "weight_file.sort()\n",
    "\n",
    "!python coco_eval.py -c 0 -p taco -w \"logs/taco/{weight_file[-1]}\" --use_custom_dataset True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "compound_coef = 0\n",
    "force_input_size = None  # set None to use default size\n",
    "\n",
    "threshold = 0.2\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "cudnn.fastest = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "obj_list = ['Trash']  # Replace with your actual object list\n",
    "\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n",
    "                             ratios=eval(params['anchors_ratios']), scales=eval(params['anchors_scales']))\n",
    "\n",
    "model.load_state_dict(torch.load(f'logs/taco/{weight_file[-1]}'))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "if use_float16:\n",
    "    model = model.half()\n",
    "\n",
    "# Load and preprocess image\n",
    "img_path = 'datasets/taco/images/batch_10000062_jpg.rf.23a022fa34f7c6d7d5369d59ca891b94.jpg'\n",
    "ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n",
    "\n",
    "if use_cuda:\n",
    "    x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "else:\n",
    "    x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features, regression, classification, anchors = model(x)\n",
    "\n",
    "    regressBoxes = BBoxTransform()\n",
    "    clipBoxes = ClipBoxes()\n",
    "\n",
    "    out = postprocess(x,\n",
    "                      anchors, regression, classification,\n",
    "                      regressBoxes, clipBoxes,\n",
    "                      threshold, iou_threshold)\n",
    "\n",
    "out = invert_affine(framed_metas, out)\n",
    "img_show = ori_imgs[0].copy()\n",
    "\n",
    "for j in range(len(out[0]['rois'])):\n",
    "    x1, y1, x2, y2 = out[0]['rois'][j].astype(int)\n",
    "    obj = obj_list[out[0]['class_ids'][j]]\n",
    "    score = float(out[0]['scores'][j])\n",
    "    plot_one_box(img_show, [x1, y1, x2, y2], label=obj, score=score,\n",
    "                 color=color_list[get_index_label(obj, obj_list)])\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img_show)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
